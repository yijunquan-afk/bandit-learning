{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit算法学习[网站优化]05——Bandit算法总结\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. White J. Bandit algorithms for website optimization[M]. \" O'Reilly Media, Inc.\", 2013.\n",
    "2. [https://github.com/johnmyleswhite/BanditsBook](https://github.com/johnmyleswhite/BanditsBook)\n",
    "\n",
    "实验环境：jupyter python 3.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、Bandit算法中的生活经验\n",
    "\n",
    "在本专栏中，我们已经了解了三种解决Multiarmed Bandit问题的算法：\n",
    "\n",
    "+ epsilon-Greedy算法\n",
    "\n",
    "+ Softmax算法\n",
    "\n",
    "+ UCB算法\n",
    "\n",
    "而通过这些算法，我们也可以汲取一些生活的经验：\n",
    "\n",
    ":one: 学会取舍\n",
    "\n",
    ":two: 上帝确实会掷骰子\n",
    "\n",
    ":three: 默认值很重要\n",
    "\n",
    ":four: 学会碰碰运气\n",
    "\n",
    ":five: 每个人都会成长\n",
    "\n",
    ":six: 让错误随风而去\n",
    "\n",
    ":seven: 胜不骄败不馁\n",
    "\n",
    ":eight: 环境十分重要\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Bandit算法的分类\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用如下6个维度衡量不同的Bandit算法：\n",
    "\n",
    "1. ***Curiosity***\n",
    "\n",
    "   这个算法会记录它对每只臂的了解程度吗？该算法是否试图显式地获取知识，而不是通过偶然获得？换句话说，这个算法是否充满好奇心？\n",
    "\n",
    "2. ***Increased Exploitation over Time***\n",
    "\n",
    "   随着时间的推移，算法是否会显式地尝试探索(explore)更少的内容？换句话说，该算法是否使用了退火法？\n",
    "\n",
    "3. ***Strategic Exploration***\n",
    "\n",
    "   哪些因素决定了算法在每个时间点的决定？它会最大化回报或知识，还是两者皆考虑？\n",
    "\n",
    "4. ***Number of Tunable Parameters***\n",
    "\n",
    "   该算法有多少个参数？通常最好使用参数较少的算法。\n",
    "\n",
    "5. ***Initialization Strategy***\n",
    "\n",
    "   该算法对尚未探索(explore)的臂的价值做出了哪些假设？\n",
    "\n",
    "6. ***Context-Aware***\n",
    "\n",
    "   算法能不能利用臂的价值的背景信息?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、其他主题的学习\n",
    "\n",
    "+ 如果想深入了解Bandit算法，可以学习关于强化学习的经典教材\n",
    "    \n",
    "    Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto, (1998)\n",
    "    \n",
    "    [pdf地址](http://incompleteideas.net/book/RLbook2020.pdf)\n",
    "\n",
    "+ 进一步学习：\n",
    "\n",
    "    实验: \"The Nonstochastic Multiarmed Bandit Prob- lem\" by Auer et al.[pdf地址](https://cseweb.ucsd.edu/~yfreund/papers/bandits.pdf)\n",
    "\n",
    "    知识梯度: \"A knowledge-gradient policy for sequential information collection\" by Frazier et al.[pdf地址](http://optimallearning.princeton.edu/Papers/FrazierPowell_KnowledgeGradientJournalPaper.04092008.pdf)\n",
    "\n",
    "    随机化概率匹配: \"A modern Bayesian look at the multiarmed bandit\" by Steven L. Scott.[pdf地址](https://www.economics.uci.edu/~ivan/asmb.874.pdf)\n",
    "\n",
    "    汤普森采样: \"An Empirical Evaluation of Thompson Sampling\" by Olivier Chapelle and Lihong Li. [pdf地址](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf)\n",
    "\n",
    "+ 上下文相关的Bandit算法:\n",
    "\n",
    "    LinUCB: \"A Contextual-Bandit Approach to Personalized News Article Recom- mendation\" by Li et al.[pdf地址](https://arxiv.org/pdf/1003.0146.pdf)\n",
    "\n",
    "    GLMUCB: \"Parametric Bandits: The Generalized Linear Case\" by Filippi et al.[pdf地址](https://papers.nips.cc/paper/2010/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.3 (tags/v3.9.3:e723086, Apr  2 2021, 11:35:20) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9fcfdf80f222a2f693b759e6b70f8329ed3ae15944fc27c696bbd5b015a4703"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
