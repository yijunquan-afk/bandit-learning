{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Multi-Armed Bandits——01 Scope and Motivation\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "1. Slivkins A. Introduction to multi-armed bandits[J]. Foundations and Trends® in Machine Learning, 2019, 12(1-2): 1-286.\n",
    "\n",
    "[Bandit算法学习[网站优化]](https://blog.csdn.net/weixin_47692652/article/details/128539899)偏实战，而本专栏偏理论学习。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandits是一个非常简单强大的算法框架，我们接下来从三个具体的例子进行学习：\n",
    "\n",
    "+ 新闻网站 \n",
    "\n",
    "    当一个新用户到达时，网站选取一个文章标题来显示，观察用户是否点击这个标题。该网站的目标是**最大限度地提高总点击量**。\n",
    "\n",
    "+ 动态定价\n",
    "\n",
    "    一家商店正在销售一种数字商品，例如，一个应用程序或一首歌。当一个新顾客到来时，商店选择一个提供给这个顾客的价格。顾客购买（或不购买），然后离开。**商店的目标是使总利润最大化**。\n",
    "\n",
    "+ 投资\n",
    "\n",
    "    每天早上，你选择一只股票，然后投资1美元。在这一天结束的时候，你观察每只股票的价值变化。**目标是使总财富最大化**。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandits结合了这些例子(以及许多其他例子)。在基本版本中，一个算法有 $K$ 种可能的动作(action)可供选择，也就是臂(arm) 和T轮。在每一轮中，算法选择一只臂，并为这只臂收集奖励(reward)。奖励是独立地从某种分布中提取的，这种分布是固定的（即只取决于所选择的臂），但不为算法所知。\n",
    "\n",
    "![image-20230113225801860](https://note-image-1307786938.cos.ap-beijing.myqcloud.com/typora/%20image-20230113225801860.png)\n",
    "\n",
    "在基本模型中，算法在每一轮之后都会观察所选臂的奖励，但不会观察其他可能被选中的臂。因此，算法通常需要探索(explore)：尝试不同的臂以获得新的信息。需要有一个探索和利用（exploration and exploitation）之间的权衡：根据现有信息做出最佳的近期决定。这种权衡在许多应用场景中都会出现，在Multi-armed bandits中至关重要。从本质上讲，该算法努力学习哪些臂是最好的，同时不花太多的时间去探索。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、多维问题空间\n",
    "\n",
    "Multi-armed bandits是一个巨大的问题空间，有许多的维度。接下来我们将讨论其中的一些建模维度。\n",
    "\n",
    "### 1.1 辅助反馈(Auxiliary feedback)\n",
    "\n",
    "在每一轮之后，除了所选臂的奖励之外，算法还能得到什么反馈?算法是否观察到其他臂的奖励?下面是一些例子:\n",
    "\n",
    "![image-20230113233624401](https://note-image-1307786938.cos.ap-beijing.myqcloud.com/typora/%20image-20230113233624401.png)\n",
    "\n",
    "我们将反馈类型分为三类:\n",
    "\n",
    "+ **bandit 反馈**\n",
    "\n",
    "    算法只能观察到所选臂的奖励，无法看到其他臂的。\n",
    "\n",
    "+ **完全反馈**\n",
    "\n",
    "    算法可以观察到所有可能被选中的臂的奖励。\n",
    "\n",
    "+ **部分反馈**\n",
    "\n",
    "    算法除了能观察到所选臂的奖励，还能观察到其他信息，但是不总是等同于完全反馈。\n",
    "\n",
    "我们主要关注bandit反馈的问题，还涵盖了一些关于完全反馈的基本结果。部分反馈有时出现在扩展和特殊情况下，可用于提高性能。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 奖励模型\n",
    "\n",
    "奖励如何产生，有以下几种方式：\n",
    "\n",
    "+ \n",
    "\n",
    "+ \n",
    "\n",
    "+ \n",
    "\n",
    "+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
